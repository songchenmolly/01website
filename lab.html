<!DOCTYPE html>
<html>
<head>
  <!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-NW63PWB');</script>
<!-- End Google Tag Manager -->
<link href="css/progress-wizard.min.css" rel="stylesheet">
	<link rel="stylesheet" type="text/css" href="css/main.css"/>
	<!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="shortcut icon" type="image/png" href="image/favicon.ico"/>
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" type="text/css" href="bootstrap/css/bootstrap.css">
	<title>Cognitive Neuroscience Lab</title>
</head>
<body>

<nav class="navbar navbar-expand-lg navbar-light bg-light">
       <div class="container">
           <a class="navbar-brand" href="/"><img src=""></a>
           <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent">
               <span class="navbar-toggler-icon"></span>
           </button>
           <div class="collapse navbar-collapse justify-content-end" id="navbarSupportedContent">
               <ul class="navbar-nav" id="headerNav">
                   <li class="nav-item" id="navPhotoPage"><a class="nav-link" href="/index.html"> Works </a></li>
                   <li class="nav-item" id="navEssayPage"><a class="nav-link" href="mollysong@outlook.com"> Contact Me </a></li>
               </ul>
           </div>
       </div>
   </nav>
 



<div class="maincontent">  
  <div class="card">
    <div class="card-header">
      <div class="cardheader">
       <B>UX Research in Emotional Expression Task (ERT)</B>
      </div>
      <div class="cardheaderintro">
        Modified ERT for Pre-lingual Cochlear Implant Children
      <br><b>Date</b>: Aug.2015 - Dec.2017
    </div>
  </div>

    
    <div class="card-body">
      <blockquote class="blockquote mb-0">
        <div class="container">
          <div class="row">

            <div class="col-sm-3">
              <b>Teammate</b><br>Amy Berglund
              <br>Holly Watkins<br>Evans Delaney<br>Meredith Scheppele
            </div>
            <div class="col-sm-3">
            <b>My role</b><br> Project Coordinator
            </div>
            <div class="col-sm-3">
              <b>Tool</b> 
              <br>Eye-tracking<br>fNIRS<br>E-prime
              <br>SPSS
            </div>
            <div class="col-sm-3">
              <b>What I have learned</b>
              <br>Project Management
              <br>Literature Review
              <br>Contextual Inquiries
              <br>Research Design
              <br>
            </div>
          </div>
        </div>
      </blockquote>
    </div>
  </div>
</div>


  <div id="problemoverview" class="maincontent">
    <h2>Background</h2>

      <p>In Child Language and Cognitive Processes Laboratory, Dr.Evans and Dr. Warner-Czyz led a grant for pre-lingual cochlear implant (CI) children's language and cognitive study. Collaborated with mentors and teammates, we conducted two studies on pre-lingual CI children's facial emotion recognition and language processing. 1 public paper, 3 conference posters.</p>
     
      <p>Here I will introduce how we chose and modified the tasks using UX research methods to test the emotional expression recognition research on pre-lingual children with cochlear implant.</p>

        <div class="paper">
        <h3>Papera Link</h3>
        <a href ="document/facial emotion.pdf">Click here</a></div>
      </div>

        <div class="maincontent">
        <h2>Problem Overview</h2>
    
      <iframe class="mx-auto d-block" width="560" height="315" src="https://www.youtube.com/embed/cUTymzn5FEc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
    </div>
     
     <div class="maincontent">
    <p>However, it is not easy to identify cochlear implant children's visual emotional expression recognition pattern scientifically. Facial expressions provide a fundamental information of effective social interaction for children with pre-lingual hearing loss experience, they rely more visual information rather than the auditory information such as lipreading.
    <p><br><b>In summary, the problem was </b>which was the proper way to measure cochlear implant emotion recognition precisely.</b> 
    <br>Under three limitations:
    <br> <b>1) Children aged from 3-10 years-old; <br>2) Children had a period of auditory deprivation in their early life;  <br>3) Children who wear the metal electrical device.</b>
</div>
      <div class="maincontent">
      <h2>Research Lifecycle</h2>
      </div>
       <div class="maincontent">
              <ul class="progress-indicator">
                  <li class="completed">
                      <span class="bubble"></span>
                      Step 1. <br><small>Literature Review</small>
                  </li>
                  <li class="completed">
                      <span class="bubble"></span>
                      Step 2. <br><small>First prototype</small>
                  </li>
                   <li class="completed">
                      <span class="bubble"></span>
                      Step 3. <br><small>User Evaluation and second prototype</small>
                  </li>
                  
                   <li class="completed">
                      <span class="bubble"></span>
                      Step 4. <br><small>Final Results</small>
                  </li>
              </ul>
            </div>
      
</div>
    <div id="literaturereview" class="maincontent">
        <h2>Literature Review</h2> 
      
        <p>Through reviewing all literature from 2008-2018 years, about developing Cochlear implant emotions development. We found that </p>
      
        <P><b>Finding 1</b>: Children's emotional expression pattern can be validated.</P>

        <p>From behavioral to neural research, there is a commonsense from behavioral studies to neuroscience studies that typical hearing children between 3-10 years old, response to happy and sad facial expressions faster and more precise than the other facial emotions. The common results from various studies indicated that visual emotion of children with an age of 3 to 10 years could be identified and consolidated by researches methods. 
        </p>
        <p><img src="image/lab/response and accuracy.png" class="figure-img img-fluid rounded"></p>

        <p><b>Finding 2</b>: Children with CI's Facial Recognition Correction Speed and Rate Pattern is similar to hearing children.</b></p>

        <p>In the behavioral study, regardless of hearing status, children with cochlear implant or the hearing children, they both recognized the happy and sad expression most quickly and correctly.<p>
        <p>Few studies had shown that children's response to active emotion is more faster than the negative. However, no studies to date have demonstrated the speed of cochlear implant children to recognize the facial emotion.</p>
        

        <p><br><b>Finding 3</b>: Emotion Recognition Task (ERT) (Montagne B, 2007) is a reliable task to identify emotion.</p>
        <p> ERT is computer-based cognition paradigm from measuring six basical emotional expressions: angry, disgust, fear, happiness, sadness, and surprise. It has been commonly used to assess subtle impairments in emotion perception.</p>
        </p>
      </p>
    </div>

        <div class="maincontent">
        <iframe src="http://www.youtube.com/embed/5rn5-Id32L4?rel=0" frameborder="0" width="560" height="315"></iframe>
     
        
      
        <P><b>First Conception</b>: we will use eye-tracking to track the emotional expression of children with cochlear implant via Montagne's emotion recognition task.  </p> 
        </div>
        <HR style="FILTER: alpha(opacity=100,finishopacity=0,style=3)" width="50%" color=#000 SIZE=3>
    <div id="ResearchProcess" class="maincontent">
      <h2>First Conception and Its Evaluation</h2>  
      
      <p><b>Period 1: Research Design</b></p>

      <b>First Conception</b>:
      <p>According to the previous research, we used 9 dots eye fixation to ensure accurate eye-tracking results. In traditional ERT, participates needed forcibly to speak out an emotion label after being presented by emotional expressions. We created the area of interests (AOI) based on each face structure (eye, nose, and mouth) to analyze a serial behavioral data such as the fixation time. In this experiment, we adapted keyboard for forcibly choice. </p>
      <p>Before doing the experiment, practice experiment had applied to ensure they really understand what they will do in the following experiment.</p>
      <p> <img src="image/lab/firstedition.png" class="figure-img img-fluid rounded">
      
      <p><b>Untargeted User Evaluation</b>:</p>
      <p><b> Participates</b>: In the research design, 5 graduate students with normal hearing had participated.</p>
      <p><b>Result</b>: The same pattern had found in the previous research. Happiness and sadness were the quickest and most corrected recognized. 
      <p><b>Feedbacks from untargeted users</b></p>
      <p>1. Using keyboard might be not easy to response.
      <br>2. The facial picture might present too fast for kids.
      </p>
      
      <HR style="FILTER: alpha(opacity=100,finishopacity=0,style=3)" width="50%" color=#afafaf SIZE=3></HR>

      <p><b>Period 2: Pilot Study</b></p>
      <p><b>Participants</b>: In 3 typical hearing, 3 cochlear implant children.</p>
      <p><b>Methods and Process</b></p>
      <p>The procedure was the same as the procedure we used in the design stage. But we replaced the keyboard with the mouse. Which participates could use the mouse to click the emotion button.</p>
      
      <p><img src="image/lab/secondedition.png" class="figure-img img-fluid rounded"></p>
      
      <p><b>Result</b>:</p>There is no distinct difference between two groups in correction. However, typical hearing children had fast response time than cochlear implant children.</p>
      <p><b>Pain Points</b>: 
        <br>*The 9 dot eye fixation took too much time for children. After returning to literature, 5 dot eye fixation is tolerable. 
        <br>* In word recognition page, the words flashed too fast. After backing to literature, we added to 1500ms.
        <br>*When analysing the AOI data, we found that children's eye movement areas were too large, so the AOI should be sectioned in large fields.
      </p>
      <p class="text-center"> <b>Final Eye-tracking Procedures</b></p>
       <p><img src="image/lab/pilotresult.png" class="figure-img img-fluid rounded"></p>

      <HR style="FILTER: alpha(opacity=100,finishopacity=0,style=3)" width="50%" color=#afafaf SIZE=3>
       <p><b>Period 3: Formal Study</b></p>

       <p><b>Participants</b></p>

        <p>58 pre-lingual cochlear implant children participated in this research who had a mean age of 9.8 years old. The average implanted cochlear age was 2.9 years old. Moreover, they all used spoken language as their first language. </p> 
        <div class="maincontent">
        <p><img src="image/lab/participants.png"  class="figure-img img-fluid rounded"></p>
        </div>
      </HR>
    </p>
  </div>

      <div id="Participant" class="maincontent">
        <p><b>Methods and Process</b>
        <p>The procedures were similar to the pilot study.</p> 
        <p>We changed the emotion words background color according to the emotion color. Also, we sectioned the area of interest into 4 rectangles.</p>
        <p><img src="image/lab/formalprocedure.png" class="figure-img img-fluid rounded" ></p>
        </div>

    </div>
    </div>

    <div id="analyse" class="maincontent">
    <h2>Analysis</h2>
    <p>All statistical analyses were carried out in SPSS Statistics Version 24. We conducted repeated measure ANOVA between factors we wanted to figure out in specific questions for each small studies.</p>
    </div>
    <div id="result" class="maincontent">
      <h2>Results</h2>  
      <p>We found the effect of auditory status and emotion type on the response time and correct of visual emotional expression recognition. Pre-lingual hearing loss children could achieve comparable facial emotion correct, but they spent more time to get the correct rate comparing to hearing children. And findings were reviewed by peers and published.</p>
     <p><img src="image/lab/finalresult.png" class="figure-img img-fluid rounded"></p>
    </div>
    
  


    
  

<div id="theend" class="maincontent1">
    <p><b>The End</b></p>



<div id="footer" class="maincontent1">
    
      <p><a href="index.html">Home</a></p>

      <p><a href="bhe.html">Behind Every Door</a>|
      <a href="endnote.html">Endnote</a>|
      <a href="uxjob.html">UX My Junior UX job</a>|
      <a href="dice.html">Dice Media</a></p>
      </div>
      </div>
</div>




</body>
</html>